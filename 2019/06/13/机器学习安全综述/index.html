<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习安全综述"><meta name="keywords" content="机器学习"><meta name="author" content="Li,undefined"><meta name="copyright" content="Li"><title>机器学习安全综述 | Stateless</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@3.6.0/css/reveal.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@3.6.0/css/theme/beige.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@3.6.0/lib/css/zenburn.min.css"></head><body><div class="reveal"><div class="slides"><section data-markdown data-separator="===" data-separator-vertical="==" data-charset="utf-8"><script type="text/template">
## 机器学习安全综述

及联邦学习简介

报告人：李赞

2019.06.11

===

# 机器学习简介

==

![机器学习解决问题的过程](/images/p3.png)

==

![机器学习的方法](/images/p4.png)

===

## 攻击模型描述

* 敌手目标——数据机密性+模型可用性

* 敌手知识——所知关于模型的信息
* 敌手能力——可以做出的攻击行为
* 敌手策略——最终采取的攻击方式

===

## 安全威胁

![机器学习过程中常见的安全威胁](/images/p8.png)

==

### 训练阶段

![投毒攻击](/images/p7.png)

==

### 措施

* 数据清洗
* 注意新收集数据集的分布变化

==

### 预测阶段——对抗攻击

精心设计对抗样本，让模型在预测阶段发生错误

==

#### 如何生成对抗样本？

优化问题、寻找最小扰动r
$$
argmin_{r}h(x+r)=l\ s.t\ x+r\in D
$$

| 符号   | 名称                  |
| ------ | --------------------- |
| x      | 输入                  |
| h(x)   | 目标模型的分类函数    |
| r      | 扰动                  |
| x*=x+r | 对抗样本   原输入分布 |
| D      | 原输入分布            |

==

### 措施

* 对抗训练：为模型注射“疫苗“

* 防御精馏：引入博弈游戏，函数化两方的行为，博弈过程中，模型稳健性不断增强

===

## 隐私威胁

![机器学习中常见的隐私威胁](/images/p6.png)

==

### 同态加密

![同态加密的过程](/images/p9.png)

==

* 加减乘除等多项式运算√
* 逻辑回归、梯度下降、简单的神经网络√
* 研究较多、有很多开源的库√
* 开销和效率存在优化空间...

==

## 差分隐私

==

### 差分攻击

假如一共100人。

攻击者通过某种方式，知道了100个人的信息，知道了99个人的信息，那么比对就可以获取第100个人的信息……

==

**找出一种方法让攻击者用某种方式查询100个信息和查询那99个信息得到的结果是一致的，那攻击者就没办法找出那第100个人的信息了**

### ——在查询结果里加入随机性

引入噪声，使至多相差1个数据的2个数据集查询结果概率不可分

==

### 但是...

模型的精度会降低。

需要平衡精度和隐私。

===

## 联邦学习

* 2016年由谷歌提出。
* 将数据“孤岛”整合到一起，在不发生隐私泄露的前提下。
* 数据本身不会移动，仅为本地目标服务，但生成的模型本身变得更好了
* 实现了“共同富裕”——“联邦”

==

![联邦学习系统构架](/images/p10.png)

===

## Reference

[1]宋蕾,马春光,段广晗.机器学习安全及隐私保护研究进展.[J].网络与信息安全学报,2018,4(08):1-11.

[2] Zhang Y and Yang Q. An Overview of Multi-Task Learning[J]. National Science Review (NSR), 2018, 5(1): 30-43.

[3] 杨强 . GDPR 对 AI 的挑战和基于联邦迁移学习的对策 [J]. 中国人工智能学会通讯 , 2018，8:1-8. 

### 鸣谢郭成老师、唐新雨学长！

===

# Thank you

## Q&A</script></section></div></div><script src="https://cdn.jsdelivr.net/npm/reveal.js@3.6.0/lib/js/head.min.js"></script><script src="https://cdn.jsdelivr.net/npm/reveal.js@3.6.0/js/reveal.min.js"></script><script>Reveal.initialize({
  mouseWheel: false,
  transition: 'slide',
  transitionSpeed: 'default',
  parallaxBackgroundImage: '',
  parallaxBackgroundSize: '',
  parallaxBackgroundHorizontal: '',
  parallaxBackgroundVertical: '',
  markdown: {
    smartypants: true
  },
  dependencies: [
    { src: '/js/third-party/reveal/marked.min.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: '/js/third-party/reveal/markdown.min.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    // Syntax highlight for <code> elements
    { src: '/js/third-party/reveal/highlight.min.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    // Zoom in and out with Alt+click
    { src: '/js/third-party/reveal/zoom.min.js', async: true },
    // Speaker notes
    { src: '/js/third-party/reveal/notes.min.js', async: true },
    // MathJax
    { src: '/js/third-party/reveal/math.min.js', async: true }
  ]
});</script></body></html>